# We're going to try putting files in `local_working_dir`, so make
# sure it exists first.
- name: Ensure local working dir exists
  delegate_to: localhost
  file:
    path: "{{ local_working_dir }}"
    state: directory

# Generate MAC addresses for the undercloud node.
- name: get MACs for the undercloud
  generate_macs:
    nodes:
      - "{{ undercloud_node }}"
    networks: "{{ networks }}"
  register: undercloud_mac_map

# Check if the undercloud volume exists.  If not, we call out to
# [fetch_image.yml](fetch_image.yml.html) to download the image.
- name: Check if undercloud volume exists
  command: >
    virsh vol-info --pool '{{ libvirt_volume_pool }}'
    '{{ undercloud_node.name }}.qcow2'
  ignore_errors: true
  changed_when: false
  register: undercloud_vol_check
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

- when: undercloud_vol_check|failed
  block:
  # Conditionally include a playbook for all the images specified
  # in options that downloads, cache and extract if tar archived
  # only if the images aren't already in volume pool
  - include: fetch_image.yml
    vars:
      image: "{{ item }}"
    with_items: "{{ images }}"

  # Conditionally include a playbook for all the images specified
  # in options that updates images to the latest delorean version
  - include: update_image.yml
    when: devmode|bool

  # inject the gating repo generated by ansible-role-tripleo-gate
  - include: inject_gating_repo.yml
    when: compressed_gating_repo is defined

  # Converts an overcloud-full.qcow2 into a undercloud.qcow2
  - include: convert_image.yml
    when: overcloud_as_undercloud|bool

  # This copies the `instackenv.json` configuration file that we
  # generated in the overcloud setup role to the undercloud host.
  - name: Copy instackenv.json to appliance
    command: >
      virt-customize -a {{ working_dir }}/undercloud.qcow2
      --upload {{ working_dir }}/instackenv.json:/home/stack/instackenv.json
      --run-command 'chown stack:stack /home/stack/instackenv.json'
    environment:
      LIBGUESTFS_BACKEND: direct
      LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

  # Copy the undercloud public key to the virthost, because we're going
  # to inject it into the undercloud image in the next task.
  - name: Copy undercloud ssh public key to working dir
    copy:
      src: "{{ undercloud_key }}.pub"
      dest: "{{ working_dir }}/id_rsa_undercloud.pub"

  # Copy the public key to `$HOME/.ssh/authorized_keys` for the `root`
  # and `stack` user on the undercloud.
  - name: Inject undercloud ssh public key to appliance
    command: >
      virt-customize -a {{ working_dir }}/undercloud.qcow2
      --mkdir {{item.homedir}}/.ssh/
      --upload '{{ working_dir }}/id_rsa_undercloud.pub:{{item.homedir}}/.ssh/authorized_keys'
      --run-command 'chown -R {{item.owner}}:{{item.group}} {{item.homedir}}/.ssh'
      --run-command 'chmod 0700 {{item.homedir}}/.ssh'
      --run-command 'chmod 0600 {{item.homedir}}/.ssh/authorized_keys'
    environment:
      LIBGUESTFS_BACKEND: direct
    with_items:
      - homedir: /root
        owner: root
        group: root
      - homedir: /home/stack
        owner: stack
        group: stack

  - name: Create undercloud customize script
    template:
      src: "{{ undercloud_customize_script }}"
      dest: "{{ working_dir}}/undercloud-customize.sh"
      mode: 0755
    when: undercloud_customize_script is defined

  # This allows to run a customization script on the
  # undercloud image, to cover any extra needs.
  - name: Perform extra undercloud customizations
    command: >
      virt-customize -a {{ working_dir }}/undercloud.qcow2
      --run '{{ working_dir }}/undercloud-customize.sh'
    when: undercloud_customize_script is defined

  # This allows to run a customization script on the
  # overcloud image, to cover any extra needs.
  - name: Perform extra overcloud customizations
    include: customize_overcloud.yml
    when: overcloud_customize_script is defined

  # Perform an SELinux relabel on the undercloud image to avoid problems
  # caused by bad labelling, since by default the undercloud runs in
  # enforcing mode.
  - name: Perform selinux relabel on undercloud image
    command: >
      virt-customize -a {{ working_dir }}/undercloud.qcow2
      --selinux-relabel
    environment:
      LIBGUESTFS_BACKEND: direct

  # Resize the undercloud image to our desired size.
  # We only do the next three tasks if using a pre-created undercloud.qcow2.
  # The convert script takes care of this for the overcloud as undercloud case.
  - name: Resize undercloud image (create target image)
    command: >
      qemu-img create -f qcow2 -o preallocation=off
      '{{ working_dir }}/undercloud-resized.qcow2'
      '{{ flavors[undercloud_node.flavor].disk }}G'
    when: not overcloud_as_undercloud|bool

  - name: Resize undercloud image (call virt-resize)
    command: >
      virt-resize --expand /dev/sda1
      '{{ working_dir }}/undercloud.qcow2'
      '{{ working_dir }}/undercloud-resized.qcow2'
    environment:
      LIBGUESTFS_BACKEND: direct
    when: not overcloud_as_undercloud|bool

  - name: Rename resized image to original name
    command: >
      mv -f '{{ working_dir }}/undercloud-resized.qcow2'
      '{{ working_dir }}/undercloud.qcow2'
    when: not overcloud_as_undercloud|bool

  # Create a libvirt volume and upload the undercloud image to
  # libvirt.
  - name: Create undercloud volume
    command: >
      virsh vol-create-as {{ libvirt_volume_pool}}
      {{ undercloud_node.name }}.qcow2
      {{ flavors[undercloud_node.flavor].disk }}G --format qcow2
    environment:
      LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

  - name: Upload undercloud volume to storage pool
    command: >
      virsh -k 0 vol-upload --pool '{{ libvirt_volume_pool }}'
      '{{ undercloud_node.name }}.qcow2'
      '{{ working_dir }}/undercloud.qcow2'
    environment:
      LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"
    async: 600
    poll: 10

# Define (but do no start) the undercloud virtual machine.
- name: Define undercloud vm
  virt:
    name: "{{ undercloud_node.name }}"
    command: define
    xml: "{{ lookup('template', 'undercloudvm.xml.j2') }}"
    uri: "{{ libvirt_uri }}"

# Start the undercloud virtual machine.
- name: Start undercloud vm
  virt:
    name: "{{ undercloud_node.name }}"
    command: start
    state: running
    uri: "{{ libvirt_uri }}"

# Configure the undercloud virtual machine to be
# automatically started at boot.
- name: Configure undercloud vm to start at virthost boot
  virt:
    name: "{{ undercloud_node.name }}"
    command: autostart
    uri: "{{ libvirt_uri }}"

# Get the ip address of the undercloud.  This will retry several times
# (`undercloud_ip_retries`) until the undercloud is ready.  The script
# works by getting the MAC address of the first undercloud interface,
# and then looking that up in the kernel ARP table.
- name: Get undercloud vm ip address
  script: "scripts/get-undercloud-ip.sh {{ undercloud_node.name }}"
  register: undercloud_vm_ip_result
  until: undercloud_vm_ip_result|success
  retries: "{{ undercloud_ip_retries }}"
  delay: 10
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

- name: Set_fact for undercloud ip
  set_fact:
    undercloud_ip: "{{ undercloud_vm_ip_result.stdout_lines[0] }}"

- name: Wait until ssh is available on undercloud node
  wait_for:
    host: "{{ undercloud_ip }}"
    state: started
    port: 22
    timeout: 300

# Add the undercloud to the in-memory inventory.
- name: Add undercloud vm to inventory
  add_host:
    name: undercloud
    groups: undercloud
    ansible_host: undercloud
    ansible_fqdn: undercloud
    ansible_user: stack
    ansible_private_key_file: "{{ undercloud_key }}"
    ansible_ssh_extra_args: '-F "{{local_working_dir}}/ssh.config.ansible"'
    undercloud_ip: "{{ undercloud_ip }}"

- name: Generate ssh configuration
  delegate_to: localhost
  template:
    src: ssh.config.j2
    dest: "{{ local_working_dir }}/ssh.config.ansible"

